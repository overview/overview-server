# Maximum number of documents to retrieve for a document set
max_documents=2000000
max_documents=${?OV_MAX_DOCUMENTS}

# Where Overview stores its (Lucene, for now) search indexes
search {
  baseDirectory: "search"
  baseDirectory: ${?OV_SEARCH_DIRECTORY}
}

# Maximum memory for each clustering.
# This will be a `java` "-Xmx" setting: e.g., "2000m"
clustering_memory=2500m
clustering_memory=${?OV_CLUSTERING_MEMORY}

ingest {
  # Number of documents to inspect at once when deciding which Step to
  # run to convert them to PDF.
  #
  # Most inspections are extremely cheap, as they only involve looking at
  # the filename of the document the user uploaded. (A ".pdf" file will
  # always be detected as application/pdf.) Longer-running inspections take
  # a small download from BlobStorage, and they can hold up the pipeline if
  # there are many converters waiting.
  #
  # A document identifier takes ~50kb RAM and not much CPU. It does not consume
  # a thread.
  n_document_identifiers=4
  n_document_identifiers=${?OV_N_DOCUMENT_IDENTIFIERS}

  # Address of HTTP work broker
  #
  # Conversion workers will connect to this address to download tasks and
  # upload results.
  broker_http_address=0.0.0.0

  # Port of HTTP work broker
  broker_http_port=9032

  # How long a worker can go without sending messages before we deem it to
  # be stalled.
  #
  # This can be quite short: slow workers can simply send "0%" progress messages
  # or keep an HTTP POST connection open to keep the task alive.
  #worker_idle_timeout=60s
  worker_idle_timeout=15s

  # Maximum allowed number of converters per file type
  #
  # HTTP converters are cheap on the server side. The limit can detect bugs,
  # though, as it will cause Overview to crash if a broken worker causes
  # denial of service by hoarding all the work to itself and never completing
  # it.
  #
  # On the _worker_ side you can start this many converters for each pipeline
  # step. The ideal worker consumes 100% CPU when it's busy, but it's rarely
  # busy if it's efficient. The ideal worker consumes 0MB RAM, but with many
  # file types that's unattainable.
  max_n_http_workers_per_step=40
  max_n_http_workers_per_step=${?OV_MAX_N_HTTP_WORKERS_PER_STEP}

  # How long the worker should long-poll for a new task.
  #
  # We _enforce_ long-polling. If the client disconnects and then the long poll
  # completes with a task, the server will think it sent that task to the client
  # -- meaning other clients won't see it until the timeout.
  #
  # Set this long enough that we don't get pounded with HTTP requests and short
  # enough that clients don't disconnect on their own.
  worker_http_create_timeout=15s

  # How many files we allow a converter to output and feed back as input into
  # another converter. The canonically-dangerous step is a zipfile full of
  # zipfiles: if the recurse buffer isn't large enough, the ingest pipeline
  # will stall.
  #
  # A file consumes about as much data as its metadata JSON: assume 5kb/doc.
  max_recurse_buffer_length=10000

  # How many documents and errors to create at once when ingesting converted
  # files.
  #
  # A larger batch means fewer SQL commands, but the transaction takes longer.
  batch_size=100

  # How long to spend batching documents for ingesting.
  #
  # A slow timeout leads to bigger batches (up to a maximum of batch_size), but
  # on average the entire file import will wait (batch_max_wait/2) after the
  # last conversion has finished.
  batch_max_wait=200ms
}

# maximum number of UTF-16 characters in a single document. Overview isn't
# designed to handle massive string sizes; if this value is too high, you may
# see an OutOfMemoryError when we fetch and tokenize 20 documents from the
# database.
max_n_chars_per_document=655360 # ought to be enough for anybody ;)
max_n_chars_per_document=${?OV_MAX_CHARS_PER_DOCUMENT}

# Clustering algorithm to use. One of:
#  KMeans
#  ConnectedComponents
#  KMeansComponents <- default
clustering_alg = KMeansComponents

# Maximum token length to cluster
#
# Any token above this number of characters (not Unicode codepoints) will
# be discarded before we begin clustering.
max_clustering_token_length=40
max_clustering_token_length=${?OV_MAX_CLUSTERING_TOKEN_LENGTH}

# Maximum memory to spend when sorting documents by metadata field values.
# This is _multiplied_ by the number of concurrent sorts.
max_mb_per_sort=200
max_mb_per_sort=${?OV_MAX_MB_PER_SORT}
n_concurrent_sorts=2
n_concurrent_sorts=${?OV_N_CONCURRENT_SORTS}

akka {
  jvm-exit-on-fatal-error: on

  actor {
    provider: remote
    guardian-supervisor-strategy: "com.overviewdocs.akkautil.FailFastSupervisorStrategyConfigurator"
    allow-java-serialization: true  # FIXME should be false
  }

  remote {
    retry-gate-closed-for: 1s

    artery {
      canonical {
        hostname: "localhost"
        hostname: ${?MESSAGE_BROKER_HOSTNAME}
        port: 9030
        port: ${?MESSAGE_BROKER_PORT}
      }
      bind {
        hostname: ""  # canonical.hostname
        hostname: ${?MESSAGE_BROKER_BIND_HOSTNAME}  # override if given
        port: ""  # canonical.port
      }
    }
  }
}

# http://doc.akka.io/docs/akka/snapshot/scala/dispatchers.html
blocking-io-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 8
  }
  throughput = 1
}
